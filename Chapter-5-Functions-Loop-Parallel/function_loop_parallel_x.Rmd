---
title: "User-defined Function, Loop, and Parallelization"
author: "AECN 396/896-002"
output:
  xaringan::moon_reader:
    # css: [default, metropolis, metropolis-fonts] 
    css: xaringan-themer.css 
    lib_dir: libs
    nature:
      # ratio: 16:10
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r, child = './../setup.Rmd'}
```

```{r, include = F}
#--- load packages ---#
suppressMessages(library(future.apply))
suppressMessages(library(parallel))
````

## Learning objectives

+ Learn how to write functions yourself
+ Learn how to use a for loop and `lapply()` to complete repetitive jobs
+ Learn how not to loop things that can be easily vectorized
+ Learn how to parallelize repetitive jobs using the `future_lapply()` function from the `future.apply` package

## Table of contents

1. [User-defined Function](#user-defined-function)
2. [Looping](#loop)
3. [Parallelization](#par)

---

class: inverse, center, middle
name: user-defined-function

# User-defined Function 

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---

# A simple function

.panelset[ 

.panel[.panel-name[When]

.left-full[

It is beneficial to write your own function when you expect to repeat the same action with different inputs to the action.

## Example: `mean()`

Calculating the average of an variable is such a common task 

+ You do not want to do `sum(x)/length(x)` every time you get a mean
+ You can just use the `mean()` function

A function is more useful when the task is longer and more complicated.

]

  ]

.panel[.panel-name[How]

.left-full[

Here is the general structure of a user-defined function:

```{r eval = F}
#--- NOT RUN ---#
function_name <- function(x) {

  1. do something on x

  2. return the results

}
```

Then, you can use the function like this:

```{r eval = F}
#--- NOT RUN ---#
function_name(x)
```

Note: the argument does not have to be named `x`

]

  ]

.panel[.panel-name[Example]

.left-full[

The following function takes numeric numbers, square them, and return the squared values: 

```{r square-it}
square_it <- function(a) {

  #--- 1. do something on x ---# 
  squared_a <- a^2

  #--- 2. return the results ---# 
  return(squared_a)

}
```

<br>

Try the function:

```{r try-square-it}
ten_squared <- square_it(10) 

ten_squared
```
]

  ] 

  <!-- panel ends here -->

.panel[.panel-name[Scope 1]

.left-full[

Any objects that are defined within a function are not created on the global environment.  

For example, `squared_a` and `original_a` are defined only internally, and are not registered on the global environment. 

```{r square-and-sqrt-it-f}
square_and_sqrt_it <- function(a) {

  #--- 1. do something on x ---# 
  squared_a <- a^2
  original_a <- sqrt(squared_a)

  #--- 2. return the results ---# 
  return(original_a)

} 
```

You can confirm this by looking at the environment tab in RStudio after running the following:

```{r square-and-sqrt-it-run}
square_and_sqrt_it(8)
```

Even though we are returning `original_a`, only its content is returned.

]

  ] 

  <!-- panel ends here -->

.panel[.panel-name[Scope 2]

.left-full[

When R sees objects that are not provided explicitly as arguments to the function, then R looks for them in the global environment:

```{r square-multiply-it-fcn}
square_multiply_it <- function(a) {

  #--- 1. do something on a ---# 
  squared_a <- a^2
  z <- multiplier * squared_a #<<

  #--- 2. return the results ---# 
  return(z)

} 
```

Here, `multiplier` is provided as an argument to the function.

Try this:

```{r square-multiply-it-run, eval = F}
square_multiply_it(10) 
```

Now, define `multiplier` on the global environment, and then run it again:

```{r square-multiply-it-run-2}
multiplier <- 10
square_multiply_it(10)  
```
]


  ] 

  <!-- panel ends here -->

.panel[.panel-name[default value]

.left-full[

You can set default values for function arguments by `argument = value` like below: 

```{r square-it-fcn}
square_it <- function(a = 5) {

  #--- 1. do something on a ---# 
  squared_a <- a^2

  #--- 2. return the results ---# 
  return(squared_a)

} 
```

Try this:

```{r }
square_it()
```

]

  ] 

  <!-- panel ends here -->

]

---

# Multiple arguments

It is easy to create a function with multiple arguments. You just simply add more arguments within `function()` like below:

```{r square-them-add-fcn}
#--- define a function with two arguments ---#
square_them_add <- function(a = 5, b = 2) {

  #--- 1. do something on a ---# 
  squared_and_summed <- a^2 + b^2

  #--- 2. return the results ---# 
  return(squared_and_summed)

} 

#--- run it ---#
square_them_add(4, 3)
```

--

As you are likely to have noticed, the order of input arguments are assumed to be the same as the order of the arguments of the function. Above,

+ `a = 4`
+ `b = 3`

You can mess with the order of input arguments if you want if you name the input arguments as follows:

```{r square-multiply-it-fcn-run}
square_them_add(b = 3, a = 4) 
```

---


# Exercises

.panelset[ 

.panel[.panel-name[Exercise 1]

.left-full[

Define a function that takes temperature in Fahrenheit, convert it into Celsius, and return it. 

Here is the formula: `temp_C <- (temp_F - 32) * 5 / 9`

]

  ]

<!-- panel ends here -->

.panel[.panel-name[Exercise 2]

.left-full[

After running a randomized nitrogen trial, you found the following relationship between corn yield (bu/acre) and nitrogen rate (lb/acre):

$$
\mbox{corn yield} = 120 + 25 \times log(\mbox{nitrogen rate})
$$ 

Write a function that takes a nitrogen rate as an argument, calculate the estimated yield for the nitrogen rate, and then return the estimated yield.

]

  ]

<!-- panel ends here -->

.panel[.panel-name[Exercise 3]

.left-full[

You would like to calculate the expected revenue as a function of nitrogen rate based on the yield response function.

Write a function that takes corn price and nitrogen rate as its arguments, calculate revenue (and yield as an intermediate step), and return revenue.

]

  ] 

  <!-- panel ends here -->

]

<!-- panel set ends here -->

---
class: inverse, center, middle
name: loop

# Loop

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---

# Loop

.panelset[ 

.panel[.panel-name[Motivations]

+ We sometimes need to run the same process over and over again often with slight changes in parameters. 
 
+ In such a case, it is very time-consuming and messy to write all of the steps one bye one. 

+ For example, suppose you are interested in knowing the square of 1 through 5 with a step of 1 ([1,2,3,4,5]). The following code certainly works:

```{r eval = F}
1^2 
2^2 
3^2 
4^2 
5^2 
```

+ However, imagine you have to do this for 1000 integers. 

+ Yes, you don’t want to write each one of them one by one as that would occupy 1000 lines of your code, and it would be time-consuming. 

+ Things will be even worse if you need to repeat much more complicated processes like Monte Carlo simulations. So, let’s learn how to write a program to do repetitive jobs effectively using loop.

  ]

.panel[.panel-name[What]

<br>

+ Looping is repeatedly evaluating the same (except parameters) process over and over again. 

+ In the example above, 
  * the same repeated process is the action of squaring 
  * what you square (parameter) changes 

## Syntax

```{r eval = F}
#--- NOT RUN ---#
for (x in a_list_of_values){
  you do what you want to do with x
} 
```


  ]

.panel[.panel-name[An example]

```{r for-loop-square}
for (x in 1:5){
  print(x^2)
}  
```

This does the same:

```{r for-loop-square-duh}
for (bluh_bluh_bluh in 1:5){
  print(bluh_bluh_bluh^2)
}  
```

  ]

.panel[.panel-name[Exercise]

<br>

Write a for loop that cubes each element of the sequence of numbers that starts from 5 and increases up to 50 with the incremental step of 5.

  ] 

]

---

# Looping with `lapply()`

.panelset[ 

.panel[.panel-name[Instruction]

<br>

Instead of using a `for` loop, we can use the `lapply()` function from the base package to loop.

## Syntax

```{r lapply-s, eval = F}
#--- NOT RUN ---#  
lapply(A, B) 
```

+ `A` is the list of values 
+ `B` is the function you would like to apply to each of the values in `A` 

**Note**:  

+ `A` is a vector, `lapply()` works on each of the vector elements
+ `A` is a list, `lapply()` works on each of the list elements whatever they may be 
+ `A` is a `data.frame`, `lapply()` works on each of the columns (`data.frame` is a list of columns of equal length)

]

.panel[.panel-name[Example 1] 

<br>

This does the same thing as the for loop example we looked at earlier:

```{r lapply-square}
lapply(1:5, function(x){x^2})  
```

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

The key difference from a for loop is the object class of the output after the loop. 

<span style="color:red"> Important</span>: the output type of `lappy()` is always a `list` (that's why it is called `lapply`)

  ] 

.panel[.panel-name[Example 2]

.left5[

**`data.frame` to use**

```{r mtcars}
mtcars %>% head()
```
]

.right5[
The code below calculate the mean of each of the variables in the `mtcars` dataset. 

.scroll-box-30[
```{r mtcars-mean}
lapply(mtcars, mean)  
```
]

]


  ] 

  <!-- panel ends here -->
.panel[.panel-name[define a function]

It is often the case that you want to write a function of the action you intend to repeat first and then loop.

For example, for the loop of squaring numbers, you can first define a function that implements the action of squaring: 

```{r square-it-fcn-define}
square_it <- function(x){
  return(x^2)
} 
```

And then loop:

```{r loop-square-lapply}
lapply(1:5, function(x) square_it(x))
```

  ] 

.panel[.panel-name[multiple arguments]

Often times, you would like to loop over a single parameter of a function that has multiple arguments: 

For example, you would like to fix the value of `b` at 5 while trying different values of `a` of the following function:

```{r square-them-add-option}
square_them_add <- function(a, b) {

  squared_and_summed <- a^2 + b^2

  return(squared_and_summed)

} 
```

Then you can do this:

.scroll-box-16[
```{r square-them-add-option-run}
lapply(1:10, function(x) square_them_add(x, b = 5)) 
```
]

As you can see, this function try each of `1:10` (called internally `x`), give it to `square_them_add()` as its first argument while `b` is fixed at 5.

  ] 

  <!-- panel ends here -->

]

---

# Exercises:

.panelset[ 

.panel[.panel-name[Exercise 1]

Use `lapply()` to cube each element of the sequence of numbers that starts from 5 and increases up to 50 with the incremental step of 5.

  ]

<!-- panel ends here -->

.panel[.panel-name[Exercise 2]

Define a function that takes a nitrogen rate as an argument and calculate the estimated yield based on the following equation:

$$
\mbox{corn yield} = 120 + 25 \times log(\mbox{nitrogen rate})
$$ 

At each value of the corn price sequence of `seq(2.5, 4.0, by = 0.1)`, calculate the revenue using `lapply()` where nitrogen rate is fixed at 200 (lb/acre).

  ]

<!-- panel ends here -->


]

<!-- panel set ends here -->

---

# Looping over multiple variables 

.panelset[ 

.panel[.panel-name[Motivations]

+ The example we have looked at is a very simple case where a loop is done over a single list of values

+ It is often the case that you want to loop over multiple variables. 
 
## Example

You are interested in understanding the sensitivity of the profitability of corn production with respect to corn price and nitrogen application rate. 

So, you would like to loop over two sets of sequences of values:

+ corn price
+ nitrogen application rate

## How

The trick is to 

+ create a `data.frame` of two (or as many variables as you would like to loop over) variables (corn price and nitrogen application rate), which stores all the permutations of the two variables 

+ then loop over the rows of the `data.frame` 

  ]

.panel[.panel-name[Example]

<br>

+ We are interested in understanding the sensitivity of corn revenue to corn price and applied nitrogen amount.

+ We consider
  * the range of $3.0/bu to $5.0/bu for corn price 
  * 0 lb/acre to 300/acre for nitrogen rate

  ] 

.panel[.panel-name[Step 1]

Get a sequence of values for corn price and nitrogen rate: 

```{r parameters}
#--- corn price vector ---#
corn_price_vec <- seq(3, 5, by = 1)

#--- nitrogen vector ---#
nitrogen_vec <- seq(0, 300, by = 100) 
```

We then create a complete combination of the values using the `expand.grid()` function, and then convert it to a `data.frame` object (this is not strictly necessary).

```{r parameters-data}
#--- crate a data.frame that holds parameter sets to loop over ---#
(
parameters_data <- expand.grid(corn_price = corn_price_vec, nitrogen = nitrogen_vec) %>% 
  #--- convert the matrix to a data.frame ---#
  data.frame()
)

```

  ] 

.panel[.panel-name[Step 2]

Define a function that 

+ takes a row number
+ refer to `parameters_data` to extract the parameters stored at the row number
+ calculate corn yield and revenue based on the extracted parameters (corn price and nitrogen rate).

```{r fcn-mult}
gen_rev_corn <- function(i) {

  #--- define corn price ---#
  corn_price <- parameters_data[i,'corn_price']

  #--- define nitrogen  ---#
  nitrogen <- parameters_data[i,'nitrogen']

  #--- calculate yield ---#
  yield <- 240 * (1 - exp(0.4 - 0.02 * nitrogen))

  #--- calculate revenue ---#
  revenue <- corn_price * yield 

  #--- combine all the information you would like to have  ---#
  data_to_return <- data.frame(
    corn_price = corn_price,
    nitrogen = nitrogen,
    revenue = revenue
  )

  return(data_to_return)
} 
```

<!-- This function 
+ takes `i` (act as a row number within the function) 
+ extract corn price and nitrogen from the `i`th row of `parameters_mat`
+ use the extracted values to calculate yield and revenue
+ create a `data.frame` of the resulting revenue, corn price, and nitrogen rate
+ returns the `data.frame` -->

  ] 

.panel[.panel-name[Step 3]

Do a loop using `lapply()`:

```{r lapply-mult}
#--- loop over all the parameter combinations ---#
rev_data <- lapply(1:nrow(parameters_data), gen_rev_corn)

#--- take a look ---#
rev_data %>% head()
```

  ] 

.panel[.panel-name[Step 4]

Combine the list of `data.frame`s into a single `data.frame` using `bind_rows()` from the `dplyr` package.

```{r bindrows}
(
final_results <- bind_rows(rev_data)
)
```

  ] 

]

---


# Tips to write a function for loop 

Before define a function, write a code that works for one row.

We will work on a specific value of `i`. Here is it `i = 1`.

```{r tips}
#--- define corn price ---#
corn_price <- parameters_data[1, 'corn_price']

#--- define nitrogen  ---#
nitrogen <- parameters_data[1, 'nitrogen']

#--- calculate yield ---#
yield <- 240 * (1 - exp(0.4 - 0.02 * nitrogen))

#--- calculate revenue ---#
revenue <- corn_price * yield 

#--- combine all the information you would like to have  ---#
data_to_return <- data.frame(
  corn_price = corn_price,
  nitrogen = nitrogen,
  revenue = revenue
)
```

After you confirm the code you write gives you desired outcomes, make it a function by replacing `1` with `i`.

---

# Exercise 

Define a function that takes nitrogen rates, corn prices, and nitrogen prices and calculate profit per acre based on the following equation:

$$
\mbox{corn yield} = 120 + 25 \times log(\mbox{nitrogen rate})
$$ 

$$
\mbox{profit} = \mbox{corn price} \times \mbox{corn yield} - \mbox{nitrogen price} \times \mbox{nitrogen rate}
$$ 

+ Fix the nitrogen rate at 200 (lb/acre)
+ Define the following sequences of numbers
  * corn price: `seq(2.5, 4.0, by = 0.05)`
  * nitrogen price: `seq(0.2, 0.6, by = 0.01)`
+ For each of the unique combination of corn price and nitrogen price, calculate the profit 
  

---

# Do you really need to loop?

.panelset[ 

<br>

.panel[.panel-name[Why not?]

.left-full[

+ Actually, we should not have used a for loop or `lapply()` in any of the examples above in practice1

+ This is because they can be easily **vectorized**. 

+ Vectorized operations are those that take vectors as inputs and work on each element of the vectors in parallel

**Example**

```{r vec-1}
#--- define numeric vectors ---#
x <- 1:1000
y <- 1:1000

#--- element wise addition ---#
z_vec <- x + y   
```

]

  ]


.panel[.panel-name[compare]

.left-full[

**Vectorized** 

```{r  vec-2}
#--- define numeric vectors ---#
x <- 1:1000
y <- 1:1000

#--- element wise addition ---#
z_vec <- x + y   
```

**Non-vectorized (loop)** 

```{r non-vec}
z_la <- lapply(1:1000, function(i) x[i] + y[i]) %>%  unlist()
```

**Compare**

```{r comp}
#--- check if identical with z_vec ---#
all.equal(z_la, z_vec) 
```

Both produce the same results. However, R is written in a way that is much better at doing vectorized operations. 


]

  ]

.panel[.panel-name[time]

.left-full[

Let's time them using the `microbenchmark()` function from the `microbenchmark` package. 

Here, we do not `unlist()` after `lapply()` to just focus on the multiplication part.

```{r benchmark-vec}
library(microbenchmark)

microbenchmark(
  #--- vectorized ---#
  "vectorized" = { x + y }, 
  #--- not vectorized ---#
  "not vectorized" = { lapply(1:1000, function(i) x[i] + y[i])},
  times = 100, 
  unit = "ms"
) 
```

+ As you can see, the vectorized version is faster. 
+ The time difference comes from R having to conduct many more internal checks and hidden operations for the non-vectorized one

]

  ]

.panel[.panel-name[vectorize 1]

<br>

Instead of this:

```{r lapply-1000, eval = F}
lapply(1:1000, square_it)
```

<br>

You can just do this:

```{r vec-1000, eval = F}
square_it(1:1000)
```

  ] 

.panel[.panel-name[vectorize 2]

<br>

Here is the vectorized version of the revenue sensitivity analysis:

```{r def-gen-rev-corn-short}
gen_rev_corn_short <- function(corn_price, nitrogen) {

  #--- calculate yield ---#
  yield <- 240 * (1 - exp(0.4 - 0.02 * nitrogen))

  #--- calculate revenue ---#
  revenue <- corn_price * yield 

  return(revenue)
} 
```

Then use the function to calculate revenue and assign it to a new variable in the parameters_data data.

```{r gen-rev-corn-short-run}
rev_data_2 <- mutate(
  parameters_data,
  revenue = gen_rev_corn_short(corn_price, nitrogen)
) 
```
 
  ] 

.panel[.panel-name[compare]

<br>

Let’s compare the vectorized and non-vectorized version:

```{r mb-v}
microbenchmark(

  #--- vectorized ---#
  "vectorized" = { rev_data <- mutate(parameters_data, revenue = gen_rev_corn_short(corn_price, nitrogen)) },

  #--- not vectorized ---#
  "not vectorized" = { parameters_data$revenue <- lapply(1:nrow(parameters_data), gen_rev_corn) },
  times = 100, 
  unit = "ms"

) 
```

  ] 

  <!-- panel ends here -->

]

---

class: inverse, center, middle
name: par

# Parallelized computation

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---

# Parallel processing 

.panelset[ 

.panel[.panel-name[Intro]

.left-full[

+ Parallelization of computation involves distributing the task at hand to multiple cores so that multiple processes are done in parallel. 

+ Our focus is on the so called **embarrassingly parallel** processes.

**Embarrassingly parallel process**: a collection of processes where each process is completely independent of any another (one process does not use the outputs of any of the other processes) 

+ The example of integer squaring is embarrassingly parallel. In order to calculate 12, you do not need to use the result of 22 or any other squares. 
 
+ Embarrassingly parallel processes are very easy to parallelize because you do not have to worry about which process to complete first to make other processes happen. 

+ Fortunately, most of the processes you are interested in parallelizing fall under this category


]

  ]

.panel[.panel-name[Instruction]

.left-full[

+ We will use the `future_lapply()` function from the `future.apply` package for parallelization. 

+ Using the package, parallelization is a piece of cake as it is basically the same syntactically as `lapply()`.

```{r eval = F}
#--- install the package ---#
install.packages(future.apply) 

#--- load packages ---#
library(future.apply) 
```

## How

You can simply replace `lapply()` with `future_lapply()`!

```{r fl-syntax, eval = F}
#--- parallelized ---#
sq_ls <- lapply(1:1000, function(x) x^2) 

#--- not parallelized ---#
sq_ls_par <- future_lapply(1:1000, function(x) x^2) 
```

]

  ]

.panel[.panel-name[Preparation]

.left-full[

+ You can find out how many cores you have available for parallel computation on your computer using the `detectCores()` function from the `parallel` package.

```{r parallel}
library(parallel)  

#--- number of all cores ---#
detectCores()
```

+ Before we implement parallelized `lapply()`, we need to declare what backend process we will be using by `plan()`. 

```{r process}
plan(multiprocess, workers = detectCores() - 1)
```

Other backend processes are:

+ `sequential`: this is just a regular loop
+ `multicore`: forked sessions (not available on Windows)
+ `multisession`: multiple sessions (less performant thana `multicore`)

With the `multiprocess` option, R figure out which `multicore` or `multisession` should be used (or can be used) and automatically redirect the backend process to the appropriate (available) one.

]

  ] 

.panel[.panel-name[Try it]

.left-full[

```{r fla-try, eval = F}
sq_ls <- future_lapply(1:1000, function(x) x^2)
```

]

  ] 

.panel[.panel-name[Any faster?]

.left-full[

```{r mb-l}
microbenchmark(

  #--- parallelized ---#
  "parallelized" = { sq_ls <- future_lapply(1:1000, function(x) x^2) }, 

  #--- non-parallelized ---#
  "not parallelized" = { sq_ls <- lapply(1:1000, function(x) x^2) },
  times = 100, 
  unit = "ms"

) 
```

]

   ]

   <!-- panel ends here --> 

.panel[.panel-name[What happened?]

.left-full[

+ This is because communicating jobs to each core takes some time as well. 

+ So, if each of the iterative processes is super fast (like this example where you just square a number), the time spent on communicating with the cores outweighs the time saving due to parallel computation.

+ Parallelization is more beneficial when each of the repetitive processes takes long.

]

  ] <!-- panel ends here -->
]

---


# Parallel processing: a less trivial example 

.panelset[ 

.panel[.panel-name[MC simulation]

.left-full[

+ One of the very good use cases of parallelization is MC simulation 

+ We will run MC simulations that test whether the correlation between an independent variable and error term would cause bias (yes, we know the answer). 

]

  ]

.panel[.panel-name[MC steps]

.left-full[

1. generate a dataset (50,000 observations) according to the following data generating process:

$$
 y = 1 + x + v
$$

where 
+ $\mu \sim N(0,1)$
+ $x \sim N(0,1) + \mu$
+ $v \sim N(0,1) + \mu$. 
 
The $\mu$ term cause correlation between $x$ (the covariate) and $v$ (the error term). 

2. estimate the coefficient on $x$ vis OLS, and return the estimate. 

3. repeat this process $1,000$ times to understand the property of the OLS estimators under the data generating process.

This Monte Carlo simulation is embarrassingly parallel because each process is independent of any other.

]
 
  ]

.panel[.panel-name[function]

.left-full[

Here is a function that implements the steps described in the previous slide:

```{r mc-sim-def}
#--- repeat steps 1-3 B times ---#
MC_sim <- function(i){

  N <- 50000 # sample size

  #--- steps 1 and 2:  ---#
  mu <- rnorm(N) # the common term shared by both x and u
  x <- rnorm(N) + mu # independent variable
  v <- rnorm(N) + mu # error
  y <- 1 + x + v # dependent variable
  data <- data.table(y = y, x = x)

  #--- OLS ---# 
  reg <- lm(y~x, data = data) # OLS

  #--- return the coef ---#
  return(reg$coef['x'])
}  
```

]

  ] 

.panel[.panel-name[performance]

.left-full[

**Single run**:

```{r s-run}
tic()
single_res <- MC_sim(1)
toc()
```

**Not parallelized (sequential)**:

```{r np-run}
tic()
MC_results <- lapply(1:1000, MC_sim)
toc() 
```

**Parallelized**:

```{r p-run}
tic()
MC_results <- future_lapply(1:1000, MC_sim)
toc() 
```

]

  ] 

  <!-- panel ends here -->
.panel[.panel-name[Mac/Linux]

.left-full[

+ For Mac or Linux users, `parallel::mclapply()` is just as compelling (or `pbmclapply::pbmclapply()` if you want to have a nice progress report, which is very helpful particularly when the process is long). 

+ It is just as easy to use as `future_lapply()` because its syntax is the same as `lapply()`. 

+ You can control the number of cores to employ by adding `mc.cores` option. Here is an example code that does the same MC simulations we conducted above: 

```{r mclapply, eval = F}
#--- mclapply ---#
library(parallel)
MC_results <- mclapply(1:1000, MC_sim, mc.cores = detectCores() - 1)

#--- or with progress bar ---#
library(pbmclapply)
MC_results <- pbmclapply(1:1000, MC_sim, mc.cores = detectCores() - 1)
``` 

]

  ] 

  <!-- panel ends here -->

]

---

# Exercise 

```{r gen-files-def, echo = F, eval = F}

gen_data <- function(i) {
  
  data <- tibble(
    N_rate = runif(1000, min = 150, max = 250) ,
    v = rnorm(1000) * 50,
    corn_yield = 100 + log(N_rate),
    field_id = 1
  )     

  saveRDS(data, paste0("../../Datasets/Chapter_5/experiment_data/corn_experiment_", i, ".rds")) 

  saveRDS(data, paste0("../../Datasets/Chapter_5/experiment_data/soy_experiment_", i, ".rds")) 

}

lapply(1:50, gen_data)

```

+ Using the `list.files()` function, get the name of all the files inside the `experiment_data` and assign it to `file_names`.

```{r get-file-names, echo = F}
file_names <- list.files("../../Datasets/Chapter_5/experiment_data")  
```

+ Using `future_lapply()`, read all the files (this is not a good use case of parallelized processing because each iteration is super fast in practice)
+ Combine all the `data.frame`s saved in a list into one `data.frame` 
+ Find the average `corn_yield` by field

---
class: inverse, center, middle
name: opt

# Low-dimensional optimization 

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---

# Low-dimensional optimization

.panelset[ 

.panel[.panel-name[setup and method]

## Setup

Suppose you have ran an randomized nitrogen experiment for corn production on a field, collected data, and run a regression to find the following quantitative relationship between corn yield (bu/acre) and nitrogen rate (lb/acre):

$$
\mbox{corn yield} = 120 + 25 \times log(\mbox{nitrogen rate})
$$

Your are interested in finding the best nitrogen rates that maximize profit at different combinations of corn and nitrogen prices for this field.

$$
Max_{N} P_C \times Y(N) - P_N \times N
$$

+ `N`: nitrogen rate (lb/acre)
+ `Y(N)`: corn yield (bu/acre) as a function of `N` as described above
+ `P_C`: corn price ($/bu)
+ `P_N`: nitrogen price ($/lb)

Here, `N` is a decision variable, and `P_C` and `P_N` are parameters.


  ]

<!-- panel ends here -->
.panel[.panel-name[grid search]

Grid search is a very inefficient yet effective tool for finding solutions to optimization problems as long as the dimension of the optimization problem is low (1 or 2 variables).

Grid search basically evaluates the objective function (profit here) at many levels of the decision variables (nitrogen here) and pick the one that maximizes the objective function (profit).


.content-box-green[**Example**]

```{r echo = F}
data.table(N = seq(0.1, 300, by = 0.1)) %>% 
  mutate(yield = 120 + 25 * log(N)) %>% 
  mutate(profit = 3.5 * yield - 0.4 * N)  
```

  ] 

  <!-- panel ends here -->
.panel[.panel-name[example]

Let's define a function that takes `N`, `P_C`, and `P_N` values and returns profits.

```{r def-get-pi}
get_profit <- function(N, P_C, P_N) {

  yield <- 120 + 25 * log(N) 
  profit <- P_C * yield - P_N * N
  return(profit)

}
```

Let's create a sequence of `N` values at which we evaluate the profit, and then calculate profit at each level of `N`.

```{r data-main}
data_main <- tibble(N = seq(0.1, 300, by = 0.1)) %>% 
  mutate(profit = get_profit(N, 3.5, 0.4))
```

```{r show-data-main, echo = F}
data_main 
```

  ]

<!-- panel ends here -->

  <!-- panel ends here -->
.panel[.panel-name[profit-N]

Here is the visualization of profit-N relationship:

```{r get-opt-n, include = F}
opt_N <- filter(data_main, profit == max(profit))$N
```  

```{r gg-data, echo = F, out.width = "60%", fig.dim = c(4,3)}
ggplot(data_main) +
  geom_line(aes(y = profit, x = N)) +
  geom_vline(xintercept = opt_N, linetype = 2, color = "red") +
  annotate('text', x = opt_N - 40, y = 450, label = "Optimal N rate", size = 3)
```


  ] 

  <!-- panel ends here -->

.panel[.panel-name[best N]

Once the profit-N relationship is found, we can use `filter()` combined with `max()` to identify the optimal N rate.

```{r show-opt-N}
filter(data_main, profit == max(profit))
```

Alternatively, you can sort the data by profit in the ascending order (default) and pick the last row using `slice(n())`.

```{r show-opt-N}
arrange(data_main, profit) %>% 
  slice(n())
```

This method is faster than the first one.

  ] 

  <!-- panel ends here -->
]

<!-- panel set ends here -->

---

# Coding strategy 1: looping

.panelset[ 

.panel[.panel-name[loop]

Now that you have written codes to find the optimal N at a given combination of corn and nitrogen prices.

We can move on to the next step of finding the optimal N rates at many various combinations of corn and nitrogen prices.

Here is the coding strategy:

1. Define a set of all the combinations of corn and nitrogen prices you want to analyze as a `data.frame`.

2. Define a function that extract corn and nitrogen prices from the parameter `data.frame` and find the optimal N rate at the given combination of price and nitrogen combination.

3. Loop over the set of parameters.

  ]

<!-- panel ends here -->

.panel[.panel-name[Step 1]

Here, we define a `data.frame` of parameters to be explored. We will be looping over the rows of the parameter `data.frame`.

```{r define-par-data}
P_C_seq <- seq(2.5, 4.5, by = 1)
P_N_seq <- seq(0.2, 0.6, by = 0.2)

price_parameters <- expand.grid(P_C = P_C_seq, P_N = P_N_seq) %>% 
  tibble()
```

Take a look:  

```{r take-a-look}
head(price_parameters) 
```
  ]

<!-- panel ends here -->
.panel[.panel-name[Step 2]

Now, we will define a function that extract a combination of corn and nitrogen prices from `price_parameters` (extract a row from `price_parameters`), and then find the optimal N.

```{r }
get_opt_N <- function(i) {

  P_C <- price_parameters[i, ]$P_C
  P_N <- price_parameters[i, ]$P_N

  N_data <- tibble(N = seq(0.1, 300, by = 0.1))

  opt_N <- mutate(N_data, profit = get_profit(N, P_C, P_N)) %>% 
    filter(profit == max(profit)) %>% 
    mutate(P_C = P_C) %>% 
    mutate(P_N = P_N)

  return(opt_N)

}  
```

Check if this function works:

```{r check-fcn}
get_opt_N(1)  
```

  ] 

  <!-- panel ends here -->

.panel[.panel-name[Step 3]

```{r parallelized-opt-N}
opt_N_all_ls <- future_lapply(1:nrow(price_parameters), get_opt_N)
```

```{r take-a-look-opt-N}
opt_N_all_ls[[1]] 
```

Combine the list of `data.frame`s into a single `data.frame` using `bind_rows()`.

```{r combine-all}
opt_N_all <- bind_rows(opt_N_all_ls)
```

```{r take-a-look-final}
head(opt_N_all) 
```
  ] 

  <!-- panel ends here -->
]

<!-- panel set ends here -->


---


# Coding strategy 2: vectorized

.panelset[ 

.panel[.panel-name[vectorized]

Instead of writing a loop like above, we can actually vectorize the process. Here are the steps:

1. Define a set of all the combinations of **nitrogen rate**, corn price, and nitrogen price you want to analyze as a `data.frame`.

2. Calculate profits for all the combinations of **nitrogen rate**, corn price, and nitrogen price inside the `data.frame`

3. Find the optimal N rate for each of the combinations of **nitrogen rate**, corn price, and nitrogen price 

  ]

<!-- panel ends here -->

.panel[.panel-name[Step 1]

Here, we define all the combinations of **nitrogen rate**, corn price, and nitrogen price you want to analyze as a `data.frame`.

```{r define-par-data-three, highlight.output = 4:6}
P_C_seq <- seq(2.5, 4.5, by = 1)
P_N_seq <- seq(0.3, 0.6, by = 0.3)
N_seq <- seq(100, 300, by = 100) 

(
price_parameters <- expand.grid(P_C = P_C_seq, P_N = P_N_seq, N = N_seq) %>% 
  tibble() %>% 
  arrange(P_C, P_N)
)
```

.red[Note]: Highlighted rows represent a single set of `P_C`-`P_N` combination with all the N rate values being explored. 

  ]

<!-- panel ends here -->
.panel[.panel-name[Steps 2 and 3]

Now, we will calculate profit for all the rows in `price_parameters`.

```{r find-pi-vec}
profit_data <- mutate(price_parameters, profit = get_profit(N, P_C, P_N))  
```

.scroll-box-12[
```{r show-pi-vec, echo = F}
profit_data 
```
]

Now, we can identify the optimal N rate at each of the corn and nitrogen combinations:

```{r show-N-vec}
profit_data %>% 
  group_by(P_C, P_N) %>% 
  arrange(profit) %>% 
  slice(n())
```


  ] 

  <!-- panel ends here -->

]

<!-- panel set ends here -->

---

# Which strategy?

Which strategy you should take depends on the size of your computer's RMA memory.

Vectorized version is more memory-hungry:

+ Strategy 1: loop over the price combinations (one price combination at a time)
+ Strategy 2: loop over price (all the price combinations at the same time)

If you can fit the entire dataset in the RAM memory, then take Strategy 2. Otherwise, break up the entire task into pieces like Strategy 1. 









